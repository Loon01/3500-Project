Machine Learning Algorithms Implementation
CS 3500 Final Project

Team Members: Solomon Anagha, Geneva Regpala, Adrian Rodriguez, Hermit Singh

Date: December 2025

================================================================================

1. INTRODUCTION

For this project, we implemented five different machine learning algorithms completely from scratch in Java. The main goal was to understand how these algorithms actually work under the hood while also practicing good object-oriented programming. We didn't use any ML libraries - everything was built using just Java's standard library.

We chose to implement:
1. Linear Regression (for regression tasks)
2. Logistic Regression (for classification)
3. k-Nearest Neighbors (classification)
4. Decision Tree using the ID3 algorithm (classification)
5. Gaussian Naive Bayes (classification)

Building these from scratch was challenging but really helped us understand the math and logic behind each algorithm. We also got to practice important software engineering concepts like interfaces, encapsulation, and modular design.

================================================================================

2. DATASET OVERVIEW

We used the Adult Census Income dataset from the UCI Machine Learning Repository. This dataset has information about people from a census and tries to predict whether someone makes more or less than $50,000 per year.

Dataset Stats:
- Total samples: 24,142 (after we cleaned it up)
- Number of features: 14 different attributes
- Target variables:
  * For classification: income (whether someone makes ≤50K or >50K)
  * For regression: hours worked per week

The features include both categorical data (like job type, education level, marital status) and numeric data (like age, capital gain, hours worked).

How We Preprocessed the Data

Getting the data ready for our algorithms took several steps:

1. One-Hot Encoding
This was needed because our algorithms work with numbers, but some features are categorical (like "Private" or "Self-employed" for work type). We converted each category into its own binary column. For example, if someone works in the Private sector, the "workclass_Private" column gets a 1 and all other workclass columns get 0s. This expanded our feature count from 14 to 108 total features.

2. Normalization
Some algorithms (like k-NN and Logistic Regression) work better when features are on the same scale. We used z-score normalization, which transforms each feature to have a mean of 0 and standard deviation of 1. The formula is: z = (x - mean) / std

We didn't normalize for all algorithms - Decision Trees and Linear Regression worked fine without it.

3. Train-Test Split
We split the data 80/20, meaning 80% for training (19,313 samples) and 20% for testing (4,829 samples). We used a random seed of 42 so we could reproduce our results. This is important because it means we're testing on data the model has never seen before.

4. Target Processing
For classification tasks, we mapped the income to binary: 0 if someone makes ≤50K and 1 if they make >50K. For regression, we just used the hours-per-week as a continuous number.

================================================================================

3. DESIGN AND IMPLEMENTATION

We organized our code into packages (models/, data/, metrics/, utils/) to keep everything clean and modular.

Key Design Decisions:

1. Model Interface - All algorithms implement the same fit(), predict(), score(), getName() methods. This was really useful because the menu system works with any model through polymorphism.

2. Encapsulation - Made all internal variables private (like weights, tree nodes) so external code can't mess with them. You have to use the public methods.

3. Separation of Concerns - Each package has one job: models/ has algorithms, data/ handles loading/preprocessing, metrics/ has evaluation functions, utils/ has low-level math.

Code: 519 lines (5 models) + 981 lines (supporting infrastructure) = ~1,500 total

OOP Concepts: Abstraction, encapsulation, polymorphism, reusability, modularity

================================================================================

4. IMPLEMENTATION DETAILS

4.1 Linear Regression

We used the closed-form solution (normal equation): weights = (X^T * X)^-1 * X^T * y

This was the simplest to implement - just matrix math. We added L2 regularization (λ = 0.01) to make sure the 108×108 matrix was invertible. Used Gauss-Jordan elimination for inversion which took some debugging.

Results: RMSE: 0.7237, R²: -0.3055 (negative R² means it's worse than just predicting the mean - makes sense since we're using it for classification when it's designed for regression)

Code: 60 lines

---

4.2 Logistic Regression

Uses sigmoid function to predict probabilities, trained with gradient descent (100 epochs, learning rate 0.1).

Getting the learning rate right was tricky - too high and it diverged, too low and it took forever. 0.1 worked well. Had to clip predictions to avoid log(0) errors.

Results: Accuracy: 83.31%, Macro-F1: 0.7086 (second best! Really close to Decision Tree)

Code: 103 lines

---

4.3 k-Nearest Neighbors

Probably the simplest conceptually - just find the 5 nearest neighbors and vote. Uses Euclidean distance. No training needed (lazy learner), just stores the data.

Main problem: super slow prediction because it calculates distance to all 16,899 training samples for each test sample. We implemented our own sorting from scratch.

Results: Accuracy: 81.20%, Macro-F1: 0.6885

Code: 88 lines

4.4 Decision Tree

This was the most complicated algorithm to implement. Decision trees recursively split the data based on features that give the most information gain.

How it works:
Recursively builds tree by splitting on features that maximize information gain.
Information Gain = Entropy(parent) - Weighted Avg Entropy(children)
Entropy: H(S) = -Σ p(c) * log₂(p(c))

Implementation:
- ID3 algorithm with histogram binning (5 bins per feature)
- Stopping conditions: max depth (10), min samples (2), pure nodes
- Custom Node class with featureIndex, threshold, left/right children
- Prediction via tree traversal

Challenges: Recursive logic with multiple base cases. Calculating entropy correctly. Handling lots of potential splits (108 features × 5 bins).

Results:
- Accuracy: 84.03%
- Macro-F1: 0.7203

BEST ACCURACY! The decision tree did the best out of all our algorithms. The tree structure works really well for this dataset because there are clear decision boundaries (like education level → income).

Hyperparameters we used:
- Max Depth: 10
- Min Samples to Split: 2
- Number of Bins: 5

Code: 168 lines (longest because of the recursion and node management)

---

4.5 Gaussian Naive Bayes

Uses Bayes' theorem assuming features are independent and follow Gaussian distributions. We used log-space to avoid numerical underflow when multiplying tiny probabilities.

Results: Accuracy: 49.95%, Macro-F1: 0.3698 (basically random guessing - WORST performer)

The problem: our one-hot encoded features are binary (0/1), not Gaussian. Plus the independence assumption is violated (education and occupation are correlated). Should've used a different Naive Bayes variant.

Code: 100 lines

================================================================================

5. EVALUATION

Metrics We Used:
- Classification: Accuracy (% correct) and Macro-F1 (balances precision and recall for both classes)
- Regression: RMSE (prediction error) and R² (how much variance we explain)

Test Setup: We used 70/30 train-test split (16,899 train / 4,829 test). All algorithms trained and tested on the same data so we could compare them fairly.

Results Summary

Algorithm               | Primary Metric    | Secondary Metric   | Training Time | Code
------------------------|-------------------|--------------------|--------------|----- 
Linear Regression       | RMSE: 10.8340     | R²: 0.1880         | 1,400 ms     | 60 lines
Logistic Regression     | Accuracy: 83.31%  | Macro-F1: 77.69%   | 700 ms       | 103 lines
k-Nearest Neighbors     | Accuracy: 81.20%  | Macro-F1: 74.79%   | 7 ms*        | 88 lines
Decision Tree           | Accuracy: 84.03%  | Macro-F1: 76.76%   | 7,700 ms     | 168 lines
Gaussian Naive Bayes    | Accuracy: 49.95%  | Macro-F1: 49.95%   | 46 ms        | 100 lines

*k-NN training is instant but prediction is slow

What We Learned From These Results

Decision Tree Won (84.03%): Captures non-linear patterns well. Entropy-based splitting found informative features. Depth limit (10) prevented overfitting.

Logistic Regression Close Second (83.31%): Surprisingly strong for a linear model. Normalized features and gradient descent worked well. Balanced F1 (77.69%).

k-NN Moderate (81.20%): Decent accuracy but very slow prediction (16,899 distance calculations per test sample).

Naive Bayes Failed (49.95%): Random guessing level. Independence assumption violated. Binary one-hot features don't fit Gaussian assumption.

Linear Regression Poor (R² = 0.188): Not suited for predicting hours-per-week. Low R² shows weak linear relationship.

Training Time: k-NN (7ms) < Naive Bayes (46ms) < Logistic Reg (700ms) < Linear Reg (1,400ms) < Decision Tree (7,700ms)

Decision Tree slowest due to evaluating many splits (108 features × 5 bins).

Confusion Matrix for Decision Tree

                    Predicted ≤50K    Predicted >50K
Actual ≤50K              3548              130
Actual >50K               642              509

Breaking this down:
- True Negatives (3548): Correctly predicted ≤50K
- False Positives (130): Predicted >50K but actually ≤50K
- False Negatives (642): Predicted ≤50K but actually >50K
- True Positives (509): Correctly predicted >50K

The model is better at identifying people who earn ≤50K (3548/3678 = 96.5%) than people who earn >50K (509/1151 = 44.2%). This is because there are way more ≤50K examples in the dataset (class imbalance).

Analysis:
- High precision for ≤50K class (96.5%)
- Lower recall for >50K class (44.2%)
- Model conservative in predicting high income

================================================================================

6. DISCUSSION

What Worked Well:
- Java's type system caught a lot of bugs during compilation instead of at runtime
- The Model interface made it easy to work with all algorithms the same way (polymorphism)
- Package structure kept code organized - when something broke we knew where to look

What Could Be Better:
- SO MUCH typing compared to other languages ("double[][]" everywhere)
- We only tested manually - should have written JUnit tests
- Java doesn't have as many ML libraries as some other languages

Extensibility: Adding new algorithms is easy - just implement the Model interface. Adding new metrics is also easy - just add static methods. Multi-class classification or online learning would be harder because our interface assumes binary batch training.

================================================================================

7. CONCLUSIONS

What We Built:
- 5 ML algorithms completely from scratch (Linear/Logistic Regression, k-NN, Decision Tree, Naive Bayes)
- Full data pipeline (CSV loading, one-hot encoding, normalization, train-test splitting)
- Evaluation metrics and an interactive menu to test everything

Key Results:
- Best performer: Decision Tree (84.03%) - the tree structure captured non-linear patterns really well
- Worst: Naive Bayes (49.95%) - random guessing level because we violated the Gaussian and independence assumptions
- Java's OOP design helped us write clean, maintainable code through interfaces and encapsulation

What We Learned:
Implementing algorithms from scratch in Java really helped us understand how they work - you can't just call a library function and hope for the best. Java was more verbose than some other languages, but it forced us to think about design and structure. The type system caught a lot of bugs early.

Stats: ~1,500 lines of code, 30-40 hours of work, accuracy ranging from 49.95% to 84.03%

================================================================================
END OF REPORT
